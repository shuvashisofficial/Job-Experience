{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Librarie"
      ],
      "metadata": {
        "id": "y_yrSgohqbt3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khsiIHu4hukh"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.19.0 accelerate torch transformers datasets pillow --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/diffusers.git\n"
      ],
      "metadata": {
        "id": "l2VhqCuxiEu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Extract the Dataset"
      ],
      "metadata": {
        "id": "UrshlIFdqghU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n"
      ],
      "metadata": {
        "id": "IIl2iTckkKi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the kaggle.json file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the kaggle.json file to the Kaggle directory\n",
        "!mv kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "RpaYurvHkOt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5IABIRznkYGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle --quiet\n"
      ],
      "metadata": {
        "id": "VfbdBaqDkbXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list\n"
      ],
      "metadata": {
        "id": "c7RIVRqMkd1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d tongpython/cat-and-dog\n",
        "!unzip cat-and-dog.zip -d data/\n"
      ],
      "metadata": {
        "id": "3Lc2-uykkmNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Dataset"
      ],
      "metadata": {
        "id": "nU6S18tdqkvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Path to dataset\n",
        "data_dir = \"data/training_set/training_set\"\n",
        "categories = os.listdir(data_dir)\n",
        "\n",
        "# Define transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "# Supported image extensions\n",
        "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}\n",
        "\n",
        "# Custom dataset class\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, data_dir, categories, transforms, valid_extensions):\n",
        "        self.data = []\n",
        "        for category in categories:\n",
        "            class_dir = os.path.join(data_dir, category)\n",
        "            for img_file in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_file)\n",
        "\n",
        "                # Check for valid image file\n",
        "                if os.path.splitext(img_file)[-1].lower() not in valid_extensions:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Load image and caption\n",
        "                    image = Image.open(img_path).convert(\"RGB\")\n",
        "                    caption = f\"A picture of a {category.lower()}.\"\n",
        "                    self.data.append((image, caption))\n",
        "                except UnidentifiedImageError:\n",
        "                    print(f\"Skipping invalid image: {img_path}\")\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, caption = self.data[idx]\n",
        "        return {\n",
        "            \"pixel_values\": self.transforms(image),\n",
        "            \"captions\": caption\n",
        "        }\n",
        "\n",
        "# Create dataset instance\n",
        "dataset = ImageCaptionDataset(data_dir, categories, image_transforms, valid_extensions)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n"
      ],
      "metadata": {
        "id": "d3P32Wz8iH0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Stable Diffusion Model"
      ],
      "metadata": {
        "id": "zDjadqSVqnyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.19.0 accelerate torch transformers --quiet\n"
      ],
      "metadata": {
        "id": "xhEPJev1ls9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "W73rrvQQmx3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers --quiet"
      ],
      "metadata": {
        "id": "DoQisXe_phJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Load Stable Diffusion pipeline for image-to-image generation\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Define a function to replace cached_download functionality\n",
        "def download_cached_model(repo_id, filename, cache_dir=None):\n",
        "    \"\"\"Downloads a file from the Hugging Face Hub, using caching if available.\"\"\"\n",
        "    return hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=cache_dir)\n",
        "\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    revision=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        "    # Use the custom download function for cached downloads\n",
        "    custom_pipeline_kwargs={\"cached_download\": download_cached_model},\n",
        ")\n",
        "\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "FA7e9-Lho4t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune the Model"
      ],
      "metadata": {
        "id": "YJk9kyqKqscu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 3\n",
        "\n",
        "# Create dataloader\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Extract model components\n",
        "unet = pipe.unet\n",
        "optimizer = AdamW(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize Accelerator\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n"
      ],
      "metadata": {
        "id": "Vl6rJ2rWqqys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "unet.train()\n",
        "for epoch in range(num_epochs):\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        # Prepare inputs\n",
        "        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch]).to(accelerator.device)\n",
        "        captions = [item[\"captions\"] for item in batch]\n",
        "\n",
        "        # Tokenize captions\n",
        "        inputs = pipe.tokenizer(captions, padding=\"max_length\", return_tensors=\"pt\", truncation=True)\n",
        "        input_ids = inputs.input_ids.to(accelerator.device)\n",
        "\n",
        "        # Generate latent noise\n",
        "        latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
        "        latents = latents * pipe.vae.config.scaling_factor\n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = unet(latents, input_ids).sample\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = torch.nn.functional.mse_loss(noise_pred, latents)\n",
        "\n",
        "        # Backpropagation\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=loss.item())\n"
      ],
      "metadata": {
        "id": "uTeYJXUmqxAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "vYqlMASVqyUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet.save_pretrained(\"./fine_tuned_unet\")\n",
        "pipe.text_encoder.save_pretrained(\"./fine_tuned_text_encoder\")\n",
        "pipe.tokenizer.save_pretrained(\"./fine_tuned_tokenizer\")\n"
      ],
      "metadata": {
        "id": "GsjD5BMMqyhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "4bNbKG6Nq3S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Reload fine-tuned components\n",
        "pipe.unet = pipe.unet.from_pretrained(\"./fine_tuned_unet\")\n",
        "pipe.text_encoder = pipe.text_encoder.from_pretrained(\"./fine_tuned_text_encoder\")\n",
        "pipe.tokenizer = pipe.tokenizer.from_pretrained(\"./fine_tuned_tokenizer\")\n",
        "\n",
        "# Test the model\n",
        "input_image = Image.open(\"data/example.jpg\").resize((512, 512))\n",
        "prompt = \"A cute dog sitting on a chair.\"\n",
        "\n",
        "output_image = pipe(prompt=prompt, init_image=input_image, strength=0.8).images[0]\n",
        "output_image.save(\"generated_image.jpg\")\n"
      ],
      "metadata": {
        "id": "befuFHBXq7jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy as a Streamlit Web App"
      ],
      "metadata": {
        "id": "AsaO43tSq_Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "# Load the fine-tuned model\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"./fine_tuned_model\").to(\"cuda\")\n",
        "\n",
        "st.title(\"Image-to-Image Generation with Stable Diffusion\")\n",
        "st.write(\"Upload an image and provide a prompt to generate a new image.\")\n",
        "\n",
        "# Upload input image\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"png\"])\n",
        "prompt = st.text_input(\"Prompt\")\n",
        "\n",
        "if st.button(\"Generate\"):\n",
        "    if uploaded_file and prompt:\n",
        "        input_image = Image.open(uploaded_file).resize((512, 512))\n",
        "        output_image = pipe(prompt=prompt, init_image=input_image, strength=0.8).images[0]\n",
        "        st.image(output_image, caption=\"Generated Image\")\n",
        "    else:\n",
        "        st.error(\"Please upload an image and provide a prompt.\")\n"
      ],
      "metadata": {
        "id": "gutbWTipq-Ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}